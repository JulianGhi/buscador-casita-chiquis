#!/usr/bin/env python3
"""
Sincroniza Google Sheets con datos scrapeados de los links.

Flujo en 3 pasos:
    1. python sync_sheet.py pull      # Descarga a data/sheet_data.json
    2. python sync_sheet.py scrape    # Scrapea links y actualiza el JSON
    3. python sync_sheet.py push      # Sube cambios a Google Sheets

Opciones de push:
    --force     Sobrescribe todo el sheet
    --merge     Solo actualiza celdas vac√≠as (default)
    --dry-run   Muestra cambios sin aplicar
"""

import argparse
import json
import re
import subprocess
import time
import unicodedata
from datetime import datetime, timedelta
from pathlib import Path

# Cargar variables de entorno desde .env
from dotenv import load_dotenv
load_dotenv()

import gspread
import httpx

# =============================================================================
# IMPORTS DE CORE - M√≥dulos refactorizados
# =============================================================================
from core import (
    # Constantes
    BARRIOS_CABA,
    ATTR_PATTERNS,
    SCOPES,
    SHEET_ID,
    WORKSHEET_NAME,
    LOCAL_FILE,
    CACHE_FILE,
    PRINTS_DIR,
    PRINTS_INDEX,
    # Helpers
    quitar_tildes,
    extraer_numero,
    extraer_m2,
    detectar_barrio,
    extraer_id_propiedad,
    get_active_rows,
    calcular_m2_faltantes,
    inferir_valores_faltantes,
    detectar_atributo,
    # Sheets API
    get_client,
    get_worksheet,
    get_cells_to_update,
    build_sheet_data,
    format_header_row,
    # Storage
    load_local_data,
    save_local_data,
    require_local_data,
    load_cache,
    save_cache,
    # Scrapers
    scrape_argenprop,
    scrape_mercadolibre,
    scrape_link,
    get_rows_to_scrape,
    apply_scraped_data,
    is_offline_error,
    # Validation
    add_warning,
    clear_warnings,
    get_warnings,
    print_warnings_summary,
    validar_propiedad,
    get_properties_with_missing_data,
    # Prints
    PRINT_DIAS_VENCIMIENTO,
    PRINT_PATTERN_ID,
    PRINT_PATTERN_FILA,
    generar_nombre_print,
    get_prints_index,
    sync_print_dates,
    build_property_index,
    extract_id_from_pdf,
    get_pending_print_files,
    process_print_file,
    get_orphan_prints,
    save_prints_index,
    clasificar_prints,
    extraer_datos_pdf,
    analizar_prints_vs_sheet,
    analizar_tres_fuentes,
    # Storage adicional
    get_cache_for_url,
    # Templates
    PREVIEW_SHOW_COLS,
    PREVIEW_DIFF_COLS,
    generate_preview_html,
    build_preview_data,
)

# =============================================================================
# CONSTANTES ESPEC√çFICAS DEL CLI (no est√°n en core/)
# =============================================================================

# Verificar que SHEET_ID existe (viene de core/ que lee de env)
if not SHEET_ID:
    raise ValueError("GOOGLE_SHEET_ID environment variable is required. Set it in .env or export it.")

# Constantes espec√≠ficas de este CLI (no est√°n en core/)
PENDIENTES_FILE = Path('data/prints/pendientes.json')

CAMPOS_IMPORTANTES = ['terraza', 'balcon', 'cocheras', 'luminosidad', 'disposicion',
                      'ascensor', 'antiguedad', 'expensas', 'banos', 'apto_credito']

SCRAPEABLE_COLS = ['precio', 'm2_cub', 'm2_tot', 'm2_desc', 'm2_terr', 'amb', 'barrio', 'direccion',
                   'expensas', 'terraza', 'antiguedad', 'apto_credito', 'tipo', 'activo',
                   'cocheras', 'disposicion', 'piso', 'ascensor', 'balcon', 'luminosidad',
                   'fecha_publicado', 'banos', 'inmobiliaria', 'dormitorios', 'fecha_print']


# =============================================================================
# COMANDOS CLI
# =============================================================================

# =============================================================================
# PULL - Descarga de Google Sheets a archivo local
# =============================================================================

def cmd_pull():
    """Descarga datos de Google Sheets a archivo local JSON"""
    print("üì• Descargando datos de Google Sheets...")

    client = get_client()
    spreadsheet = client.open_by_key(SHEET_ID)

    try:
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME)
    except gspread.WorksheetNotFound:
        worksheet = spreadsheet.sheet1

    all_values = worksheet.get_all_values()

    if not all_values:
        print("‚ùå Sheet vac√≠o")
        return

    headers = [h.lower().strip() for h in all_values[0]]
    rows = []

    for i, row_values in enumerate(all_values[1:], start=2):
        row = {'_row': i}  # Guardar n√∫mero de fila original
        for h, v in zip(headers, row_values):
            row[h] = v
        rows.append(row)

    # Guardar a archivo
    LOCAL_FILE.parent.mkdir(exist_ok=True)

    data = {
        'headers': headers,
        'rows': rows,
        'source': f'Google Sheet {SHEET_ID}',
        'pulled_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(LOCAL_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ {len(rows)} filas guardadas en {LOCAL_FILE}")

    # Stats
    with_price = sum(1 for r in rows if r.get('precio', '').strip())
    with_m2 = sum(1 for r in rows if r.get('m2_cub', '').strip())
    with_link = sum(1 for r in rows if r.get('link', '').strip())
    print(f"\nüìä Estad√≠sticas:")
    print(f"   Con precio: {with_price}/{len(rows)}")
    print(f"   Con m¬≤: {with_m2}/{len(rows)}")
    print(f"   Con link: {with_link}/{len(rows)}")


# =============================================================================
# SCRAPE - Scrapea links y actualiza archivo local
# =============================================================================

def cmd_scrape(check_all=False, no_cache=False, force_update=False):
    """Scrapea links del archivo local y actualiza los datos

    Args:
        check_all: Scrapear todos los links (no solo los que faltan datos)
        no_cache: Ignorar cache y re-scrapear
        force_update: Sobrescribir valores existentes (no solo llenar vac√≠os)
    """
    if not LOCAL_FILE.exists():
        print(f"‚ùå No existe {LOCAL_FILE}")
        print("   Ejecut√° primero: python sync_sheet.py pull")
        return

    with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    headers = data['headers']
    rows = data['rows']

    # Cargar cache y encontrar filas a scrapear
    cache = load_cache() if not no_cache else {}
    to_scrape = get_rows_to_scrape(rows, check_all)

    if not to_scrape:
        print("‚úÖ No hay filas que necesiten scraping")
        return

    print(f"üîç Scrapeando {len(to_scrape)} links...")
    if not no_cache:
        print(f"   (usando cache de {len(cache)} links)")
    if force_update:
        print(f"   ‚ö†Ô∏è  Modo --update: sobrescribiendo valores existentes")

    # Contadores
    updated, offline, cache_hits = 0, 0, 0
    clear_warnings()

    for idx, row in to_scrape:
        link = row.get('link', '')
        direccion = row.get('direccion', '(sin direcci√≥n)')[:35]
        row_num = row.get('_row', idx + 2)
        print(f"   Fila {row_num}: {direccion}...")

        scraped, from_cache = scrape_link(link, use_cache=not no_cache, cache=cache)

        if scraped is None:
            print(f"      ‚è≠Ô∏è  Dominio no soportado")
            continue

        if from_cache:
            cache_hits += 1
            print(f"      üì¶ Cache", end='')

        # Manejar errores
        if '_error' in scraped:
            print(f"      ‚ùå {scraped['_error']}")
            if is_offline_error(scraped) and 'activo' in headers:
                # Solo guardar fecha_inactivo si es la primera vez que se marca como inactivo
                era_activo = rows[idx].get('activo', '').lower() != 'no'
                rows[idx]['activo'] = 'no'
                if era_activo and 'fecha_inactivo' in headers:
                    from datetime import datetime
                    rows[idx]['fecha_inactivo'] = datetime.now().strftime('%Y-%m-%d')
                    print(f"      üì¥ Marcado como NO activo (vendida {rows[idx]['fecha_inactivo']})")
                else:
                    print(f"      üì¥ Marcado como NO activo")
                offline += 1
            continue

        # Link activo - marcar y aplicar datos
        if 'activo' in headers:
            rows[idx]['activo'] = 'si'

        result = apply_scraped_data(rows[idx], scraped, SCRAPEABLE_COLS, headers, force_update)

        if result['changes']:
            print(f"      ‚úÖ Nuevo: {', '.join(result['changes'])}")
            updated += 1
        if result['updates']:
            print(f"      üîÑ Actualizado: {', '.join(result['updates'])}")
            updated += 1
        if not result['changes'] and not result['updates']:
            print(f"      ‚ö™ Sin cambios")

        validar_propiedad(rows[idx], contexto=direccion)
        time.sleep(0.5)

    # Calcular m2 faltantes y aplicar inferencias a todas las filas
    m2_calculados = 0
    inferencias_total = 0
    for row in rows:
        # Calcular m2 faltantes (si tenemos 2 de 3)
        m2_calc = calcular_m2_faltantes(row)
        for campo, valor in m2_calc.items():
            row[campo] = valor
        m2_calculados += len(m2_calc)

        # Inferir valores faltantes
        inferidos = inferir_valores_faltantes(row)
        for campo, valor in inferidos.items():
            row[campo] = valor
        inferencias_total += len(inferidos)

    if m2_calculados:
        print(f"üìê {m2_calculados} m¬≤ calculados (cub/tot/desc)")
    if inferencias_total:
        print(f"üß† {inferencias_total} valores inferidos (status, cochera, ascensor, etc.)")

    # Guardar cambios
    data['rows'] = rows
    data['scraped_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
    with open(LOCAL_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    if not no_cache:
        save_cache(cache)

    # Resumen
    print(f"\n‚úÖ {updated} filas actualizadas en {LOCAL_FILE}")
    if cache_hits:
        print(f"üì¶ {cache_hits} desde cache, {len(to_scrape) - cache_hits} scrapeados")
    if offline:
        print(f"üì¥ {offline} links marcados como NO activos")
    print_warnings_summary()
    print(f"\n   Revis√° con: python sync_sheet.py view")
    print(f"   Sub√≠ con: python sync_sheet.py push")


# =============================================================================
# PUSH - Sube archivo local a Google Sheets
# =============================================================================

def cmd_push(force=False, dry_run=False):
    """Sube los datos locales a Google Sheets"""
    if not LOCAL_FILE.exists():
        print(f"‚ùå No existe {LOCAL_FILE}")
        print("   Ejecut√° primero: python sync_sheet.py pull")
        return

    with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    headers = data['headers']
    rows = data['rows']

    # Sincronizar fechas de prints antes de push
    prints_updated = sync_print_dates(rows)

    # Guardar JSON local con fechas actualizadas
    if prints_updated:
        save_local_data(data)

    mode = "FORCE (sobrescribe todo)" if force else "MERGE (solo celdas vac√≠as)"
    print(f"üì§ {'[DRY RUN] ' if dry_run else ''}Push en modo {mode}...")
    print(f"   {len(rows)} filas a procesar")
    if prints_updated:
        print(f"   üì∏ {prints_updated} fechas de print sincronizadas")

    if dry_run:
        print("\n   Esto es un dry-run, no se aplicar√°n cambios.")
        return

    client = get_client()
    spreadsheet = client.open_by_key(SHEET_ID)
    try:
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME)
    except gspread.WorksheetNotFound:
        worksheet = spreadsheet.sheet1

    if force:
        # Force: sobrescribir todo
        all_data = build_sheet_data(headers, rows)
        worksheet.clear()
        worksheet.update(values=all_data, range_name='A1')
        format_header_row(worksheet)
        print(f"‚úÖ Sheet sobrescrito con {len(rows)} filas")
    else:
        # Merge: solo actualizar celdas que cambiaron
        current_values = worksheet.get_all_values()
        cells = get_cells_to_update(rows, current_values, headers, SCRAPEABLE_COLS)

        if cells:
            worksheet.update_cells(cells)
            print(f"‚úÖ {len(cells)} celdas actualizadas")
        else:
            print("‚úÖ No hay cambios para aplicar")


# =============================================================================
# DIFF - Muestra diferencias entre local y cloud
# =============================================================================

# ANSI colors
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
RESET = '\033[0m'
BOLD = '\033[1m'
DIM = '\033[2m'


def cmd_diff():
    """Muestra diferencias entre datos locales y Google Sheets"""
    if not LOCAL_FILE.exists():
        print(f"‚ùå No existe {LOCAL_FILE}")
        print("   Ejecut√° primero: python sync_sheet.py pull")
        return

    with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
        local_data = json.load(f)

    print("üìä Descargando datos actuales de Google Sheets para comparar...")

    client = get_client()
    spreadsheet = client.open_by_key(SHEET_ID)
    try:
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME)
    except gspread.WorksheetNotFound:
        worksheet = spreadsheet.sheet1

    cloud_values = worksheet.get_all_values()
    cloud_headers = [h.lower().strip() for h in cloud_values[0]]
    cloud_rows = {}
    for i, row in enumerate(cloud_values[1:], start=2):
        cloud_rows[i] = dict(zip(cloud_headers, row))

    local_rows = local_data['rows']
    headers = local_data['headers']

    # Campos a comparar
    DIFF_COLS = ['precio', 'm2_cub', 'm2_tot', 'amb', 'direccion', 'barrio']

    def fmt_val(local_val, cloud_val, width=8):
        """Formatea valor con color seg√∫n el cambio"""
        local_val = str(local_val or '').strip()
        cloud_val = str(cloud_val or '').strip()

        if not cloud_val and local_val:
            return f"{GREEN}{local_val:<{width}}{RESET}"  # Agregado
        elif cloud_val and local_val and local_val != cloud_val:
            return f"{YELLOW}{local_val:<{width}}{RESET}"  # Modificado
        elif not local_val:
            return f"{DIM}{'-':<{width}}{RESET}"  # Vac√≠o
        return f"{local_val:<{width}}"  # Sin cambio

    print()
    print(f"{BOLD}Comparaci√≥n: Local vs Google Sheets{RESET}")
    print(f"{GREEN}‚ñ† Verde = Nuevo{RESET}  {YELLOW}‚ñ† Amarillo = Modificado{RESET}  Sin color = Sin cambio")
    print()
    print(f"{'Fila':>4} ‚îÇ {'Direcci√≥n':<20} ‚îÇ {'Barrio':<12} ‚îÇ {'Precio':>8} ‚îÇ {'m¬≤c':>4} ‚îÇ {'m¬≤t':>4} ‚îÇ {'Amb':>3}")
    print('‚îÄ' * 78)

    added_cells = 0
    modified_cells = 0

    for row in local_rows:
        fila = row.get('_row', 0)
        if fila < 2:
            continue

        cloud = cloud_rows.get(fila, {})

        # Solo mostrar filas con alg√∫n dato
        has_data = any(row.get(c) for c in DIFF_COLS)
        if not has_data:
            continue

        # Contar cambios
        for col in ['precio', 'm2_cub', 'm2_tot', 'amb']:
            local_val = str(row.get(col, '') or '').strip()
            cloud_val = str(cloud.get(col, '') or '').strip()
            if local_val and not cloud_val:
                added_cells += 1
            elif local_val and cloud_val and local_val != cloud_val:
                modified_cells += 1

        dir_val = fmt_val(row.get('direccion', '')[:20], cloud.get('direccion', ''), 20)
        barrio_val = fmt_val(row.get('barrio', '')[:12], cloud.get('barrio', ''), 12)
        precio_val = fmt_val(row.get('precio', ''), cloud.get('precio', ''), 8)
        m2c_val = fmt_val(row.get('m2_cub', ''), cloud.get('m2_cub', ''), 4)
        m2t_val = fmt_val(row.get('m2_tot', ''), cloud.get('m2_tot', ''), 4)
        amb_val = fmt_val(row.get('amb', ''), cloud.get('amb', ''), 3)

        print(f"{fila:>4} ‚îÇ {dir_val} ‚îÇ {barrio_val} ‚îÇ {precio_val} ‚îÇ {m2c_val} ‚îÇ {m2t_val} ‚îÇ {amb_val}")

    print()
    print(f"{BOLD}Resumen:{RESET}")
    print(f"  {GREEN}+ {added_cells} celdas nuevas{RESET}")
    print(f"  {YELLOW}~ {modified_cells} celdas modificadas{RESET}")

    if added_cells or modified_cells:
        print(f"\n  Ejecut√° {BOLD}python sync_sheet.py push{RESET} para aplicar cambios")


# =============================================================================
# VIEW - Genera HTML para visualizar en browser
# =============================================================================

def check_link_status(url):
    """Verifica si un link est√° online"""
    if not url or not url.startswith('http'):
        return None
    try:
        resp = httpx.head(url, follow_redirects=True,
                         headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)
        # Detectar redirect de MercadoLibre a b√∫squeda
        final_url = str(resp.url)
        if 'redirectedFromVip' in final_url:
            return 410  # Marcar como dado de baja
        if 'mercadolibre' in url and 'MLA-' in url and 'MLA-' not in final_url:
            return 410  # Redirect a b√∫squeda = no disponible
        return resp.status_code
    except:
        return 0


def cmd_view(check_links=False):
    """Genera un HTML con los datos locales vs cloud para ver en browser"""
    if not LOCAL_FILE.exists():
        print(f"‚ùå No existe {LOCAL_FILE}")
        print("   Ejecut√° primero: python sync_sheet.py pull")
        return

    with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
        local_data = json.load(f)

    print("üìä Descargando datos actuales de Google Sheets...")

    client = get_client()
    spreadsheet = client.open_by_key(SHEET_ID)
    try:
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME)
    except gspread.WorksheetNotFound:
        worksheet = spreadsheet.sheet1

    cloud_values = worksheet.get_all_values()
    cloud_headers = [h.lower().strip() for h in cloud_values[0]]
    cloud_rows = {}
    for i, row in enumerate(cloud_values[1:], start=2):
        cloud_rows[i] = dict(zip(cloud_headers, row))

    local_rows = local_data['rows']

    # Verificar links si se pidi√≥
    link_status = {}
    if check_links:
        links_to_check = [(row.get('_row'), row.get('link', ''))
                         for row in local_rows if row.get('link', '').startswith('http')]
        print(f"üîç Verificando {len(links_to_check)} links...")
        for i, (row_num, url) in enumerate(links_to_check):
            status = check_link_status(url)
            link_status[row_num] = status
            icon = '‚úì' if status == 200 else '‚úó' if status in [404, 410] else '?'
            print(f"   [{i+1}/{len(links_to_check)}] {icon} {status} - {url[:50]}...")
            time.sleep(0.3)

    # Generar datos y HTML usando templates
    rows_data, stats = build_preview_data(
        local_rows, cloud_rows, link_status,
        columns=PREVIEW_SHOW_COLS, diff_cols=PREVIEW_DIFF_COLS
    )
    html = generate_preview_html(rows_data, stats, columns=PREVIEW_SHOW_COLS)

    # Guardar HTML
    html_path = LOCAL_FILE.parent / 'preview.html'
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html)

    print(f"‚úÖ Preview generado: {html_path}")

    # Abrir en browser
    subprocess.run(['xdg-open', str(html_path)])


# =============================================================================
# SISTEMA DE PRINTS - Comandos CLI
# =============================================================================

def cmd_prints_open(limit=None):
    """Abre en el browser todas las propiedades sin print."""
    import webbrowser

    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    rows = data['rows']
    prints_index = get_prints_index(rows)

    # Encontrar propiedades activas sin print
    sin_print = []
    for row in rows:
        fila = row.get('_row', 0)
        if fila < 2:
            continue
        activo = (row.get('activo') or '').lower()
        if activo == 'no':
            continue
        link = row.get('link', '')
        if not link.startswith('http'):
            continue
        if fila in prints_index:
            continue  # Ya tiene print

        sin_print.append({
            'fila': fila,
            'link': link,
            'direccion': row.get('direccion', ''),
            'barrio': row.get('barrio', ''),
        })

    if not sin_print:
        print("‚úÖ Todas las propiedades activas tienen print!")
        return

    # Limitar cantidad si se especifica
    to_open = sin_print[:limit] if limit else sin_print

    print(f"\nüåê Abriendo {len(to_open)} pesta√±as...")
    print(f"   (Guard√° cada PDF con Ctrl+P, el nombre que quieras)")
    print(f"   (Despu√©s ejecut√°: python sync_sheet.py prints scan)\n")

    for p in to_open:
        print(f"   ‚Üí {p['direccion'][:40]} ({p['barrio']})")
        webbrowser.open(p['link'])
        time.sleep(0.3)  # Peque√±a pausa entre tabs

    print(f"\nüìÅ Guard√° los PDFs en: {(PRINTS_DIR / 'nuevos').absolute()}")


def cmd_prints_scan():
    """Analiza PDFs nuevos en la carpeta 'nuevos/', extrae IDs y los mueve a prints/."""
    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    # Construir √≠ndices y listar archivos
    id_to_fila, fila_to_info = build_property_index(data['rows'])

    NUEVOS_DIR = PRINTS_DIR / 'nuevos'
    NUEVOS_DIR.mkdir(parents=True, exist_ok=True)
    PRINTS_DIR.mkdir(parents=True, exist_ok=True)

    archivos = get_pending_print_files(NUEVOS_DIR)
    if not archivos:
        print("‚úÖ No hay archivos nuevos para procesar")
        print(f"   (Guard√° los PDFs en: {NUEVOS_DIR.absolute()})")
        return

    print(f"\nüîç Analizando {len(archivos)} archivos...")

    procesados = []
    sin_match = []

    for archivo in archivos:
        print(f"\n   üìÑ {archivo.name[:50]}...")

        result = process_print_file(archivo, id_to_fila, fila_to_info)
        if result:
            print(f"      ‚úÖ Match: Fila {result['fila']} - {result['direccion'][:30]}")
            print(f"      ‚Üí Renombrado a: {result['archivo_nuevo']}")
            procesados.append(result)
        else:
            print(f"      ‚ùå No se encontr√≥ match")
            # Intentar mostrar ID detectado
            if archivo.suffix.lower() == '.pdf':
                prop_id = extract_id_from_pdf(archivo)
                if prop_id:
                    print(f"         ID detectado: {prop_id} (no est√° en el sheet)")
            sin_match.append(archivo.name)

    # Resumen
    print(f"\n{'='*60}")
    print(f"üìä RESUMEN")
    print(f"{'='*60}")
    print(f"   Procesados: {len(procesados)} ‚úÖ")
    print(f"   Sin match: {len(sin_match)} ‚ùå")

    if procesados:
        print(f"\n‚úÖ RENOMBRADOS:")
        for p in procesados:
            print(f"   Fila {p['fila']:2d}: {p['direccion'][:35]} ‚Üí {p['archivo_nuevo']}")

    if sin_match:
        print(f"\n‚ùå SIN MATCH (revisar manualmente):")
        for s in sin_match:
            print(f"   {s}")
        print(f"\n   Tip: Verific√° que las propiedades est√©n en el sheet")
        print(f"        o renombr√° manualmente con formato: MLA123456_2025-12-15.pdf")


def cmd_prints():
    """Muestra estado de prints: cu√°les existen, cu√°les faltan, cu√°les est√°n vencidos."""
    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    # Clasificar usando funci√≥n de core/
    c = clasificar_prints(data['rows'])
    activas, con_print, sin_print = c['activas'], c['con_print'], c['sin_print']
    vencidos, actualizados = c['vencidos'], c['actualizados']

    print(f"\nüì∏ ESTADO DE PRINTS")
    print(f"{'='*70}")
    print(f"   Propiedades activas: {len(activas)}")
    print(f"   Con print actualizado: {len(actualizados)} ‚úÖ")
    print(f"   Con print vencido (>{PRINT_DIAS_VENCIMIENTO}d): {len(vencidos)} ‚ö†Ô∏è")
    print(f"   Sin print: {len(sin_print)} ‚ùå")
    print(f"{'='*70}")

    if vencidos:
        print(f"\n‚ö†Ô∏è  PRINTS VENCIDOS (actualizar):")
        for p in vencidos:
            print(f"   Fila {p['fila']:2d}: {p['direccion'][:35]:<35} | {p['print']['archivo'][:30]} ({p['print']['dias']}d)")

    if sin_print:
        print(f"\n‚ùå SIN PRINT (crear):")
        for p in sin_print[:15]:
            id_str = p['prop_id'] or 'SIN_ID'
            print(f"   {id_str:<15} {p['direccion'][:30]:<30} ‚Üí {p['nombre_sugerido'] or 'N/A'}")
        if len(sin_print) > 15:
            print(f"   ... y {len(sin_print) - 15} m√°s")

    if actualizados:
        print(f"\n‚úÖ PRINTS ACTUALIZADOS:")
        for p in actualizados[:10]:
            print(f"   Fila {p['fila']:2d}: {p['direccion'][:35]:<35} | {p['print']['archivo'][:30]} ({p['print']['dias']}d)")
        if len(actualizados) > 10:
            print(f"   ... y {len(actualizados) - 10} m√°s")

    # Detectar hu√©rfanos y guardar √≠ndice
    filas_activas = {p['fila'] for p in activas}
    huerfanos = get_orphan_prints(c['prints_index'], filas_activas)

    if huerfanos:
        print(f"\nüì¶ PRINTS DE PROPIEDADES INACTIVAS ({len(huerfanos)}):")
        for h in huerfanos[:8]:
            print(f"   {h}")
        if len(huerfanos) > 8:
            print(f"   ... y {len(huerfanos) - 8} m√°s")
        print(f"   (pueden moverse a sin_asociar/ si ya no sirven)")

    save_prints_index(c, c['prints_index'], huerfanos, PRINTS_INDEX)
    print(f"\nüíæ √çndice guardado en: {PRINTS_INDEX}")

    if sin_print or vencidos:
        print(f"\nüí° SUGERENCIAS:")
        if sin_print:
            print(f"   ‚Üí Crear prints para {len(sin_print)} propiedades sin respaldo")
        if vencidos:
            print(f"   ‚Üí Actualizar {len(vencidos)} prints vencidos (pueden haber cambiado)")
        print(f"   ‚Üí Nomenclatura: {{ID}}_{{FECHA}}.pdf (ej: MLA123456_2025-12-15.pdf)")


def cmd_prints_validate():
    """Valida datos del sheet contra los PDFs guardados (sin scrapear online)."""
    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    rows = data['rows']

    print(f"\nüîç VALIDANDO PDFs vs SHEET")
    print(f"{'='*70}")

    # Analizar todos los PDFs
    resultados = analizar_prints_vs_sheet(rows, PRINTS_DIR)

    if not resultados:
        print(f"‚úÖ No se encontraron discrepancias entre PDFs y sheet")
        print(f"   (Solo se analizan propiedades con PDF guardado)")
        return

    print(f"‚ö†Ô∏è  Encontradas {len(resultados)} propiedades con diferencias:\n")

    for r in resultados:
        print(f"üìÑ Fila {r['fila']}: {r['direccion'][:40]}")
        print(f"   Archivo: {r['archivo']}")

        v = r['validacion']

        if v['discrepancias']:
            print(f"   ‚ùå DISCREPANCIAS:")
            for d in v['discrepancias']:
                if isinstance(d.get('diff'), str):
                    print(f"      - {d['campo']}: PDF={d['pdf']} vs Sheet={d['sheet']} ({d['diff']})")
                else:
                    print(f"      - {d['campo']}: PDF={d['pdf']} vs Sheet={d['sheet']}")

        if v['faltantes_sheet']:
            print(f"   üìù DATOS EN PDF, FALTA EN SHEET:")
            for campo in v['faltantes_sheet']:
                valor_pdf = r['datos_pdf'].get(campo)
                print(f"      - {campo}: {valor_pdf}")

        if v['coincidencias']:
            print(f"   ‚úÖ Coinciden: {', '.join(v['coincidencias'][:5])}")

        print()

    print(f"{'='*70}")
    print(f"üí° Tip: Los datos del PDF son una snapshot. Si hay discrepancias,")
    print(f"        el aviso pudo haber cambiado o el scraper extrajo mal.")

    # Sugerir import si hay datos faltantes
    total_faltantes = sum(len(r['validacion']['faltantes_sheet']) for r in resultados)
    if total_faltantes:
        print(f"\nüí° Hay {total_faltantes} campos que se pueden importar desde los PDFs.")
        print(f"   Ejecut√°: python sync_sheet.py prints import")


def cmd_prints_compare():
    """Muestra comparaci√≥n detallada: Sheet vs Web Cache vs PDF para cada propiedad."""
    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    rows = data['rows']

    print(f"\nüìä COMPARACI√ìN: SHEET vs WEB CACHE vs PDF")
    print(f"{'='*90}")

    # Usar la nueva funci√≥n que compara las 3 fuentes
    resultados = analizar_tres_fuentes(rows, PRINTS_DIR)

    if not resultados:
        print(f"‚úÖ No hay diferencias entre las fuentes")
        return

    # Contadores
    total_importar = 0
    total_revisar = 0
    total_solo_pdf = 0
    total_solo_web = 0
    cache_viejo = False

    for r in resultados:
        # Filtrar solo los que tienen diferencias
        diffs = [c for c in r['comparaciones'] if c['accion'] != 'ok']
        if not diffs:
            continue

        # Header de la propiedad
        web_info = ""
        if r['web_age'] is not None:
            if r['web_stale']:
                web_info = f" {YELLOW}(cache {r['web_age']}d){RESET}"
                cache_viejo = True
            else:
                web_info = f" ({r['web_age']}d)"

        pdf_info = " üìÑ" if r['tiene_pdf'] else f" {DIM}(sin PDF){RESET}"

        print(f"\nüìç Fila {r['fila']}: {r['direccion'][:40]}{web_info}{pdf_info}")
        print(f"   {'Campo':<12} ‚îÇ {'Sheet':<10} ‚îÇ {'Web':<10} ‚îÇ {'PDF':<10} ‚îÇ Acci√≥n")
        print(f"   {'‚îÄ'*12}‚îÄ‚îº‚îÄ{'‚îÄ'*10}‚îÄ‚îº‚îÄ{'‚îÄ'*10}‚îÄ‚îº‚îÄ{'‚îÄ'*10}‚îÄ‚îº‚îÄ{'‚îÄ'*18}")

        for c in r['comparaciones']:
            accion = c['accion']

            # Formatear valores
            v_sheet = str(c['sheet'])[:10] if c['sheet'] else '-'
            v_web = str(c['web'])[:10] if c['web'] else '-'
            v_pdf = str(c['pdf'])[:10] if c['pdf'] else '-'

            # Colorear seg√∫n acci√≥n
            if accion == 'importar':
                estado = f"{GREEN}‚Üê IMPORTAR{RESET}"
                total_importar += 1
            elif accion == 'solo_pdf':
                estado = f"{GREEN}‚Üê solo PDF{RESET}"
                total_solo_pdf += 1
            elif accion == 'solo_web':
                estado = f"{GREEN}‚Üê solo Web{RESET}"
                total_solo_web += 1
            elif accion == 'revisar':
                estado = f"{YELLOW}‚ö† REVISAR{RESET}"
                total_revisar += 1
            elif accion == 'desactualizado':
                estado = f"{YELLOW}‚ö† DESACTUALIZADO{RESET}"
                total_revisar += 1
            else:
                estado = f"{DIM}‚úì OK{RESET}"

            print(f"   {c['campo']:<12} ‚îÇ {v_sheet:<10} ‚îÇ {v_web:<10} ‚îÇ {v_pdf:<10} ‚îÇ {estado}")

    print(f"\n{'='*90}")
    print(f"üìã RESUMEN:")
    if total_importar:
        print(f"   {GREEN}‚óè Alta confianza (Web=PDF): {total_importar} campos{RESET}")
    if total_solo_pdf:
        print(f"   {GREEN}‚óè Solo en PDF: {total_solo_pdf} campos{RESET}")
    if total_solo_web:
        print(f"   {GREEN}‚óè Solo en Web: {total_solo_web} campos{RESET}")
    if total_revisar:
        print(f"   {YELLOW}‚óè Revisar manualmente: {total_revisar} campos{RESET}")

    total_importables = total_importar + total_solo_pdf + total_solo_web
    if total_importables:
        print(f"\nüí° Para importar {total_importables} campos: python sync_sheet.py prints import")
    if total_revisar:
        print(f"‚ö†Ô∏è  Los {total_revisar} campos marcados 'REVISAR' requieren revisi√≥n manual")
    if cache_viejo:
        print(f"\n‚ö†Ô∏è  Algunos datos de Web Cache tienen >7 d√≠as. Consider√° re-scrapear:")
        print(f"   python sync_sheet.py scrape --all --no-cache")


def cmd_prints_import(dry_run=False):
    """Importa datos donde hay consenso entre fuentes (Web=PDF o √∫nica fuente)."""
    data = load_local_data()
    if not data:
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    rows = data['rows']
    headers = data['headers']

    # Analizar las 3 fuentes
    resultados = analizar_tres_fuentes(rows, PRINTS_DIR)

    if not resultados:
        print(f"‚úÖ No hay datos para importar")
        return

    # Separar cambios por confianza
    rows_by_fila = {r['_row']: r for r in rows}
    cambios_alta = []     # Web y PDF coinciden
    cambios_media = []    # Solo una fuente
    cambios_revisar = []  # Discrepancias (no importar)

    for r in resultados:
        fila = r['fila']
        row = rows_by_fila.get(fila)
        if not row:
            continue

        for c in r['comparaciones']:
            if c['accion'] == 'ok' or not c['valor_sugerido']:
                continue

            campo = c['campo']
            if campo not in headers:
                continue

            cambio = {
                'fila': fila,
                'direccion': r['direccion'],
                'campo': campo,
                'valor': str(c['valor_sugerido']),
                'web': c['web'],
                'pdf': c['pdf'],
                'row': row
            }

            if c['accion'] == 'importar':
                cambios_alta.append(cambio)
            elif c['accion'] in ('solo_pdf', 'solo_web'):
                cambios_media.append(cambio)
            elif c['accion'] in ('revisar', 'desactualizado'):
                cambios_revisar.append(cambio)

    if not cambios_alta and not cambios_media:
        if cambios_revisar:
            print(f"‚ö†Ô∏è  Hay {len(cambios_revisar)} campos con discrepancias que requieren revisi√≥n manual")
            print(f"   Ejecut√°: python sync_sheet.py prints compare")
        else:
            print(f"‚úÖ No hay campos para importar")
        return

    # Mostrar preview
    print(f"\nüì• IMPORTAR DATOS VALIDADOS")
    print(f"{'='*80}")

    if cambios_alta:
        print(f"\n{GREEN}‚úÖ ALTA CONFIANZA (Web y PDF coinciden): {len(cambios_alta)} campos{RESET}")
        by_fila = {}
        for c in cambios_alta:
            if c['fila'] not in by_fila:
                by_fila[c['fila']] = {'direccion': c['direccion'], 'campos': []}
            by_fila[c['fila']]['campos'].append(f"{c['campo']}={c['valor']}")
        for fila, info in by_fila.items():
            print(f"   Fila {fila}: {info['direccion'][:35]}")
            print(f"      + {', '.join(info['campos'])}")

    if cambios_media:
        print(f"\n{YELLOW}‚ö° CONFIANZA MEDIA (√∫nica fuente): {len(cambios_media)} campos{RESET}")
        by_fila = {}
        for c in cambios_media:
            if c['fila'] not in by_fila:
                by_fila[c['fila']] = {'direccion': c['direccion'], 'campos': []}
            fuente = "PDF" if c['pdf'] else "Web"
            by_fila[c['fila']]['campos'].append(f"{c['campo']}={c['valor']} ({fuente})")
        for fila, info in by_fila.items():
            print(f"   Fila {fila}: {info['direccion'][:35]}")
            print(f"      + {', '.join(info['campos'])}")

    if cambios_revisar:
        print(f"\n{RED}‚ùå NO SE IMPORTAR√ÅN ({len(cambios_revisar)} discrepancias):{RESET}")
        for c in cambios_revisar[:5]:
            print(f"   Fila {c['fila']}: {c['campo']} ‚Üí Web={c['web'] or '-'}, PDF={c['pdf'] or '-'}")
        if len(cambios_revisar) > 5:
            print(f"   ... y {len(cambios_revisar) - 5} m√°s")

    print(f"\n{'='*80}")

    total = len(cambios_alta) + len(cambios_media)

    if dry_run:
        print(f"üìã [DRY RUN] Se importar√≠an {total} campos")
        print(f"   Ejecut√° sin --dry-run para aplicar")
        return

    # Pedir confirmaci√≥n
    print(f"\n¬øImportar {total} campos? [s/N]: ", end='')
    try:
        respuesta = input().strip().lower()
    except EOFError:
        respuesta = 'n'

    if respuesta != 's':
        print("‚ùå Cancelado")
        return

    # Aplicar cambios
    for c in cambios_alta + cambios_media:
        c['row'][c['campo']] = c['valor']

    save_local_data(data)
    print(f"\n‚úÖ Importados {total} campos")
    print(f"   Guardado en: {LOCAL_FILE}")
    print(f"\n   Revis√° con: python sync_sheet.py view")
    print(f"   Sub√≠ con: python sync_sheet.py push --force")


def cmd_pendientes(solo_sin_print=False):
    """Genera lista de propiedades con datos faltantes."""
    if not LOCAL_FILE.exists():
        print("‚ùå Primero ejecut√°: python sync_sheet.py pull")
        return

    with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    rows = data['rows']
    prints_index = get_prints_index(rows)

    # Obtener propiedades con datos faltantes
    pendientes = get_properties_with_missing_data(
        rows, CAMPOS_IMPORTANTES, prints_index, solo_sin_print
    )

    # Guardar JSON
    PRINTS_DIR.mkdir(parents=True, exist_ok=True)
    con_print = sum(1 for p in pendientes if p['tiene_print'])
    sin_print = len(pendientes) - con_print

    output = {
        'total': len(pendientes),
        'con_print': con_print,
        'sin_print': sin_print,
        'instrucciones': 'Guard√° los screenshots en data/prints/ con el nombre: fila_XX.pdf o el t√≠tulo del aviso',
        'propiedades': pendientes
    }

    with open(PENDIENTES_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    # Mostrar resumen
    print(f"\nüìã PROPIEDADES CON DATOS FALTANTES")
    print(f"{'='*60}")
    print(f"   Total: {len(pendientes)}")
    print(f"   Con print: {con_print} ‚úÖ")
    print(f"   Sin print: {sin_print} ‚ö†Ô∏è")
    print(f"{'='*60}\n")

    for p in pendientes:
        print_icon = '‚úÖ' if p['tiene_print'] else '‚ö†Ô∏è'
        missing_str = ', '.join(p['missing'][:5])
        if len(p['missing']) > 5:
            missing_str += f' +{len(p["missing"])-5}'
        print(f"   {print_icon} Fila {p['fila']:2d}: {p['direccion'][:30]:<30} | Faltan: {missing_str}")

    print(f"\nüíæ Guardado en: {PENDIENTES_FILE}")
    print(f"üì∏ Tip: Guard√° PDFs con Ctrl+P ‚Üí 'Guardar como PDF'")


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Sincroniza Google Sheets con datos scrapeados',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Flujo de trabajo:
    python sync_sheet.py pull            # 1. Descargar de Google Sheets
    python sync_sheet.py scrape          # 2. Scrapear links faltantes
    python sync_sheet.py view            # 3. Ver preview en browser
    python sync_sheet.py diff            # 3. Ver cambios en terminal
    python sync_sheet.py push            # 4. Subir cambios (merge)
    python sync_sheet.py push --force    # 4. Subir sobrescribiendo todo
    python sync_sheet.py prints          # 5. Ver estado de prints/backups
    python sync_sheet.py prints validate # 5. Validar PDFs vs sheet (offline)
    python sync_sheet.py prints compare  # 5. Comparar Sheet vs Web Cache vs PDF
    python sync_sheet.py prints import   # 5. Importar datos con consenso de fuentes
    python sync_sheet.py pendientes      # 6. Ver props con datos faltantes
        """
    )

    parser.add_argument('command', choices=['pull', 'scrape', 'view', 'diff', 'push', 'prints', 'pendientes'],
                       help='Comando a ejecutar')
    parser.add_argument('subcommand', nargs='?', default=None,
                       help='[prints] Subcomando: open, scan, validate, compare, import')
    parser.add_argument('--force', action='store_true',
                       help='[push] Sobrescribe todo el sheet')
    parser.add_argument('--dry-run', action='store_true',
                       help='[push] Muestra cambios sin aplicar')
    parser.add_argument('--check-links', action='store_true',
                       help='[view] Verifica si los links est√°n online')
    parser.add_argument('--all', action='store_true',
                       help='[scrape] Scrapea todos los links (no solo los que faltan datos)')
    parser.add_argument('--no-cache', action='store_true',
                       help='[scrape] Ignora el cache y re-scrapea todo')
    parser.add_argument('--update', action='store_true',
                       help='[scrape] Sobrescribe valores existentes (no solo llena vac√≠os)')
    parser.add_argument('--sin-print', action='store_true',
                       help='[pendientes] Solo muestra los que no tienen screenshot')
    parser.add_argument('--limit', type=int, default=None,
                       help='[prints open] Limita cantidad de tabs a abrir')

    args = parser.parse_args()

    if args.command == 'pull':
        cmd_pull()
    elif args.command == 'scrape':
        cmd_scrape(check_all=args.all, no_cache=args.no_cache, force_update=args.update)
    elif args.command == 'view':
        cmd_view(check_links=args.check_links)
    elif args.command == 'diff':
        cmd_diff()
    elif args.command == 'push':
        cmd_push(force=args.force, dry_run=args.dry_run)
    elif args.command == 'prints':
        if args.subcommand == 'open':
            cmd_prints_open(limit=args.limit)
        elif args.subcommand == 'scan':
            cmd_prints_scan()
        elif args.subcommand == 'validate':
            cmd_prints_validate()
        elif args.subcommand == 'compare':
            cmd_prints_compare()
        elif args.subcommand == 'import':
            cmd_prints_import(dry_run=args.dry_run)
        else:
            cmd_prints()
    elif args.command == 'pendientes':
        cmd_pendientes(solo_sin_print=args.sin_print)


if __name__ == '__main__':
    main()
